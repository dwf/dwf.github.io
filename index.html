<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="style.css">
    <title>David Warde-Farley</title>
  </head>
  <body>
    <div class="container">
        <h1>David Warde-Farley</h1>
        <h2>Deep Learning &amp; Deep Reinforcement Learning Researcher</h2>
	<hr>
        <p>I am a Senior Research Scientist at <a href="http://deepmind.com/">DeepMind</a> in London, England. I completed my PhD research in 2016 under <a href="http://mila.quebec/en/yoshua-bengio/">Yoshua Bengio</a> at Université de Montréal as part of (what shortly thereafter became) <a href="http://mila.quebec/">Mila</a>. Prior to that, I completed Bachelor's and Master's degrees at <a href="http://wwww.cs.toronto.edu/">the Department of Computer Science</a> at the University of Toronto, the latter supervised by <a href="http://www.morrislab.ca/">Quaid Morris</a>.</p>
        <p>My work currently focuses on reinforcement learning in sparse reward settings, more specifically unsupervised/self-supervised objectives for control. I am also interested in core deep learning methods, in particular unsupervised representation learning and likelihood-free approaches to generative models.</p>
        <p>You can find out more about my past research from <a href="https://scholar.google.com/citations?user=MOgfm8oAAAAJ&hl=en">Google Scholar</a>, and find some code of varying vintages that I've written on my (mostly inactive) <a href="https://github.com/dwf">GitHub</a> page. You can also find me on <a href="http://twitter.com/dwf">Twitter</a> and <a href="https://www.linkedin.com/in/david-warde-farley-55a0825/">LinkedIn</a>.</p>
        <hr>
        <footer>Copyright &copy; 2019 David Warde-Farley. Powered by <a href="http://www.vim.org/">Vim</a> and far too much espresso.</footer>
    </div>
  </body>
</html>
